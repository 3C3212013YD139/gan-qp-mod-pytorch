# gan-qp-mod-pytorch
A gan-qp implementation modified by me  
Only 128x128 img size.  

I am training in my anime avatar dataset.  
Maybe my dataset is too small(3.7w img) and the effect is not good.  

The last layer of the generator of Model 1 does not use any activation functions.  
There are variegated color in the resulting image.  
Later replaced with the Tanh activation function, the variegated color is gone. 

The samples folder contains images generated by my training process.  

model 1  
![model 1](https://github.com/One-sixth/gan-qp-mod-pytorch/blob/master/samples/test_16000.jpg)

model 2
![model 2](https://github.com/One-sixth/gan-qp-mod-pytorch/blob/master/samples2/test_12800.jpg)

# Modify place
Model 1  
Fewer convolution kernel.  
Added minibatch_stddev layer.  
Fixup init.  

Model 2  
Fewer convolution kernel.  
Added minibatch_stddev layer.  
Fixup init.  
Replace DeConv with Upsample+Conv.  

# Generate an image
if you want to try model 1
just run
```
python3 test.py
```

if you want to try model 2
just run
```
python3 test2.py
```

# Train on you dataset
For Model 1
Delete all weight.
edit file gan-qp-torch.py
change
```
img_dir = r'../datasets/faces/*.jpg'
```
to
```
img_dir = r'your/datasets/path/*.jpg'
```
change batch_size (line 144. Requires batch_size is multiple of 4 because minibatch_stddev.
if you VRAM is 3GB, I suggest set to 12
save file
run
```
python3 gan-qp-torch.py
```
Then the training started.
Model 2 operations are similar to the above.
